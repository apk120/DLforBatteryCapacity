{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "train_df = pd.read_excel('train.xlsx')\n",
    "test_df = pd.read_excel('test.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cycles = list(set(np.array(train_df['Cycle_Index'])))\n",
    "test_cycles = list(set(np.array(test_df['Cycle_Index'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_voltage = []\n",
    "train_current = []\n",
    "train_temp = []\n",
    "train_charge_cap = []\n",
    "train_discharge_cap = []\n",
    "for i in train_cycles:\n",
    "    dat = train_df[train_df['Cycle_Index'] == i]\n",
    "    train_voltage.append(np.array(dat['Voltage(V)']).tolist())\n",
    "    train_current.append(np.array(dat['Current(A)']).tolist())\n",
    "    train_temp.append(np.array(dat['Aux_Temperature(℃)_1']).tolist())\n",
    "    train_charge_cap.append(np.array(dat['Charge_Capacity(Ah)']).tolist())\n",
    "    train_discharge_cap.append(np.array(dat['Discharge_Capacity(Ah)']))\n",
    "    \n",
    "#maximum length of time series is 362, so prepend zeros to make same length\n",
    "train_data =[]\n",
    "d1 = []\n",
    "d2 =[]\n",
    "d3 =[]\n",
    "d4 = []\n",
    "for i in range(0, len(train_cycles)):\n",
    "    d1 = (list(np.zeros(362-len(train_voltage[i]))) + list(train_voltage[i]))\n",
    "    d2 = (list(np.zeros(362-len(train_current[i]))) + list(train_current[i]))\n",
    "    d3 = (list(np.zeros(362-len(train_charge_cap[i]))) + list(train_charge_cap[i]))\n",
    "    d4 = list(np.zeros(362-len(train_temp[i]))) + list(train_temp[i])\n",
    "    d = [d1, d2, d3, d4]\n",
    "    train_data.append(np.transpose(d))\n",
    "train_data = np.array(train_data)\n",
    "\n",
    "\n",
    "train_labels = [train_discharge_cap[i][-1] for i in range(0, len(train_discharge_cap))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(171, 362, 4)\n"
     ]
    }
   ],
   "source": [
    "print (np.shape(train_data))\n",
    "#print (train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_voltage = []\n",
    "test_current = []\n",
    "test_temp = []\n",
    "test_charge_cap = []\n",
    "test_discharge_cap = []\n",
    "for i in test_cycles:\n",
    "    dat = test_df[test_df['Cycle_Index'] == i]\n",
    "    test_voltage.append(np.array(dat['Voltage(V)']).tolist())\n",
    "    test_current.append(np.array(dat['Current(A)']).tolist())\n",
    "    test_temp.append(np.array(dat['Aux_Temperature(℃)_1']).tolist())\n",
    "    test_charge_cap.append(np.array(dat['Charge_Capacity(Ah)']).tolist())\n",
    "    test_discharge_cap.append(np.array(dat['Discharge_Capacity(Ah)']))\n",
    "    \n",
    "#maximum length of time series is 362, so prepend zeros to make same length\n",
    "test_data =[]\n",
    "d1 = []\n",
    "d2 =[]\n",
    "d3 =[]\n",
    "d4 = []\n",
    "for i in range(0, len(test_cycles)):\n",
    "    d1 = (list(np.zeros(362-len(test_voltage[i]))) + list(test_voltage[i]))\n",
    "    d2 = (list(np.zeros(362-len(test_current[i]))) + list(test_current[i]))\n",
    "    d3 = (list(np.zeros(362-len(test_charge_cap[i]))) + list(test_charge_cap[i]))\n",
    "    d4 = list(np.zeros(362-len(test_temp[i]))) + list(test_temp[i])\n",
    "    d = [d1, d2, d3, d4]\n",
    "    test_data.append(np.transpose(d))\n",
    "test_data = np.array(test_data)\n",
    "\n",
    "\n",
    "test_labels = [test_discharge_cap[i][-1] for i in range(0, len(test_discharge_cap))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43, 362, 4)\n"
     ]
    }
   ],
   "source": [
    "print (np.shape(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor = torch.tensor(train_data)\n",
    "test_tensor = torch.tensor(test_data)\n",
    "train_label = torch.tensor(train_labels)\n",
    "test_label = torch.tensor(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train_label.view(171, 1) \n",
    "test_label = test_label.view(43, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([171, 1])\n",
      "torch.Size([43, 1])\n",
      "(171, 362, 4)\n"
     ]
    }
   ],
   "source": [
    "print (train_label.shape)\n",
    "print (test_label.shape)\n",
    "print(np.shape(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the LSTM model\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_size = 4, seq_len =362, hidden_layer_size = 16, num_layers = 1, output_size = 1, batch_size = 1):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        \n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, num_layers, batch_first = True)\n",
    "        \n",
    "        self.linear1 = nn.Linear(seq_len*hidden_layer_size, 128)\n",
    "        \n",
    "        self.linear2 = nn.Linear(128, 64)\n",
    "        \n",
    "        self.linear3 = nn.Linear(64, 16)\n",
    "        \n",
    "        self.linear4 = nn.Linear(16, 1)\n",
    "        \n",
    "        self.hidden_cell = (torch.zeros(num_layers, batch_size , self.hidden_layer_size), \n",
    "                            torch.zeros(num_layers, batch_size, self.hidden_layer_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        lstm_out, self.hidden_cell = self.lstm(x, self.hidden_cell)\n",
    "        \n",
    "        lstm_out = lstm_out.reshape(1, -1)\n",
    "        \n",
    "        out = self.linear1(lstm_out)\n",
    "        \n",
    "        out = self.linear2(out)\n",
    "        \n",
    "        out = self.linear3(out)\n",
    "        \n",
    "        out = self.linear4(out)\n",
    "        \n",
    "        \n",
    "        return out[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss,self).__init__()\n",
    "\n",
    "    def forward(self,x,y):\n",
    "        criterion = nn.MSELoss()\n",
    "        loss = torch.sqrt(criterion(x, y))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMNet()\n",
    "criterion = RMSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMNet(\n",
      "  (lstm): LSTM(4, 16, batch_first=True)\n",
      "  (linear1): Linear(in_features=5792, out_features=128, bias=True)\n",
      "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (linear3): Linear(in_features=64, out_features=16, bias=True)\n",
      "  (linear4): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.258668828428837 2.0310011935788532\n",
      "1 1.8859710728215893 1.6373879216438116\n",
      "2 1.4552177448718868 1.1776339231535446\n",
      "3 0.953324542408101 0.6447467665339626\n",
      "4 0.4087290784768891 0.1844145419985749\n",
      "5 0.13001056721335963 0.14448073852893917\n",
      "6 0.10529708653165583 0.13450955512911775\n",
      "7 0.09534620820430287 0.12524981276933536\n",
      "8 0.08907900079649095 0.11645566308221152\n",
      "9 0.08338423012292873 0.1097571239915005\n",
      "10 0.07706657696885673 0.11048365470974944\n",
      "11 0.07707081900702582 0.10515037248300951\n",
      "12 0.07427535837853862 0.09914682632268862\n",
      "13 0.07005789056856032 0.09084346405295439\n",
      "14 0.06548823529516744 0.08487100933873376\n",
      "15 0.06205662858416462 0.08025609615237214\n",
      "16 0.058736163970322636 0.07699384800223417\n",
      "17 0.05676253218399851 0.07255531189053557\n",
      "18 0.05531669848146494 0.06690026161282561\n",
      "19 0.05286384116836459 0.06323064205258391\n",
      "20 0.050728227659972785 0.061163173165432246\n",
      "21 0.04974774101324249 0.05779755669970845\n",
      "22 0.04840702550453052 0.05426992094793985\n",
      "23 0.04770407760352419 0.051278408183607946\n",
      "24 0.04637852816553841 0.05032146254251169\n",
      "25 0.04541540215587058 0.04948478798533595\n",
      "26 0.0450723519799305 0.045842367549275245\n",
      "27 0.044206947611089335 0.0451137991838677\n",
      "28 0.04305288387320892 0.04477617352507835\n",
      "29 0.0427181337311951 0.04426489597143129\n",
      "30 0.04157937200445878 0.04197224628093631\n",
      "31 0.04095047468330428 0.041791364204051884\n",
      "32 0.04078264682613618 0.0382182099098383\n",
      "33 0.039810887554235626 0.03811183918354123\n",
      "34 0.03894338342878553 0.03425188120021377\n",
      "35 0.038428319127936116 0.03362334606259368\n",
      "36 0.037256797851874815 0.03282207943672358\n",
      "37 0.037398194011889006 0.03176638137462527\n",
      "38 0.035957964540225026 0.030977903410445814\n",
      "39 0.0355154708114981 0.03142145899839179\n",
      "40 0.03493163669318484 0.029444101244904274\n",
      "41 0.03403290809943662 0.029832266097845034\n",
      "42 0.03402868000387448 0.02791097829508227\n",
      "43 0.033162222968207464 0.0279356601626374\n",
      "44 0.03246085197604888 0.026661764743716218\n",
      "45 0.03139323030996044 0.0268526132716689\n",
      "46 0.030961070144385622 0.0259406705235326\n",
      "47 0.030670853386148375 0.02549608918123467\n",
      "48 0.029291198964704546 0.025686652161354243\n",
      "49 0.02923820520702161 0.025290494741395463\n",
      "50 0.02861789165184512 0.024247990098110465\n",
      "51 0.028014241603382846 0.02342583135116932\n",
      "52 0.027266203311451694 0.023766772691593614\n",
      "53 0.02667901251051161 0.022722884666088017\n",
      "54 0.026046974617138244 0.022170424461364746\n",
      "55 0.025881946435448718 0.021233489347058675\n",
      "56 0.02533001579039278 0.021158867104108945\n",
      "57 0.02511944129453068 0.019961207412010015\n",
      "58 0.024474558077360455 0.019394076147744823\n",
      "59 0.024255436066298458 0.018816016441167788\n",
      "60 0.023601695808053713 0.01826383069504139\n",
      "61 0.02320466125220583 0.0184833615325218\n",
      "62 0.022420617572048253 0.017661740613538167\n",
      "63 0.02192356572513692 0.017551047857417616\n",
      "64 0.022163998313814576 0.018508079440094704\n",
      "65 0.021168481536776 0.01821238218351852\n",
      "66 0.021025023962322035 0.017638630645219668\n",
      "67 0.02004539896870217 0.017520050669825354\n",
      "68 0.019806619276080215 0.017432276592698207\n",
      "69 0.02001063586675633 0.018199765404989552\n",
      "70 0.019600272178649902 0.019990580026493517\n",
      "71 0.019041916083174144 0.019809487254120583\n",
      "72 0.01887766729321396 0.019843650418658588\n",
      "73 0.017995926371791905 0.016481252603752668\n",
      "74 0.018078503552933185 0.01666606858719227\n",
      "75 0.017564680841233995 0.014629480450652366\n",
      "76 0.017332660524468673 0.015072193256644316\n",
      "77 0.01728659833383839 0.0178592371386151\n",
      "78 0.016962041631776687 0.0157509820405827\n",
      "79 0.017072905573928564 0.017464598944020825\n",
      "80 0.016908649115534555 0.014901075252266817\n",
      "81 0.016712257736607602 0.01580891221068626\n",
      "82 0.016740428076850042 0.014183022255121275\n",
      "83 0.01644928134672823 0.014920653298843739\n",
      "84 0.0163327413692809 0.015740968460260435\n",
      "85 0.016510506122432954 0.017044408376826796\n",
      "86 0.01609132931246395 0.014967264131058094\n",
      "87 0.01636403833913524 0.016175810680832972\n",
      "88 0.016113020523249755 0.01820317534513252\n",
      "89 0.016223100193759853 0.017868818238724108\n",
      "90 0.016063259358991656 0.01691629443057748\n",
      "91 0.015928173622889827 0.017983686092287995\n",
      "92 0.01559645111797846 0.016256964483926464\n",
      "93 0.015774529579787228 0.018454845561537633\n",
      "94 0.015477880399826675 0.018086580343024676\n",
      "95 0.015493547707273249 0.017778673837351246\n",
      "96 0.015310657651800858 0.017464474190113155\n",
      "97 0.015515044418691892 0.018230316250823265\n",
      "98 0.01509327707234879 0.017049520514732183\n",
      "99 0.01509724932107312 0.01746531419975813\n"
     ]
    }
   ],
   "source": [
    "loss_list_train = []\n",
    "loss_list_test = []\n",
    "for epoch in range(0, 100):\n",
    "    loss_list = []\n",
    "    for i in range(0, len(train_tensor)):\n",
    "        \n",
    "        #forward pass\n",
    "        model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                        torch.zeros(1, 1, model.hidden_layer_size))\n",
    "        output = model(train_tensor[i].unsqueeze(0).float())\n",
    "        loss = criterion(output, train_label[i])\n",
    "        loss_list.append(loss.item())\n",
    "        #backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss_list_train.append(sum(loss_list)/len(loss_list))\n",
    "    \n",
    "    #prediction\n",
    "    with torch.no_grad():\n",
    "        loss_list =[]\n",
    "        for i in range(0, len(test_tensor)):\n",
    "            \n",
    "            test_out = model((test_tensor[i].unsqueeze(0)).float())\n",
    "            test_loss = criterion(test_out, test_label[i])\n",
    "            loss_list.append(test_loss.item())\n",
    "        loss_list_test.append(sum(loss_list)/len(loss_list))\n",
    "    print (epoch, loss_list_train[-1], loss_list_test[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbbb32668d0>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list_train)\n",
    "plt.plot(loss_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0310011935788532, 1.6373879216438116, 1.1776339231535446, 0.6447467665339626, 0.1844145419985749, 0.14448073852893917, 0.13450955512911775, 0.12524981276933536, 0.11645566308221152, 0.1097571239915005, 0.11048365470974944, 0.10515037248300951, 0.09914682632268862, 0.09084346405295439, 0.08487100933873376, 0.08025609615237214, 0.07699384800223417, 0.07255531189053557, 0.06690026161282561, 0.06323064205258391, 0.061163173165432246, 0.05779755669970845, 0.05426992094793985, 0.051278408183607946, 0.05032146254251169, 0.04948478798533595, 0.045842367549275245, 0.0451137991838677, 0.04477617352507835, 0.04426489597143129, 0.04197224628093631, 0.041791364204051884, 0.0382182099098383, 0.03811183918354123, 0.03425188120021377, 0.03362334606259368, 0.03282207943672358, 0.03176638137462527, 0.030977903410445814, 0.03142145899839179, 0.029444101244904274, 0.029832266097845034, 0.02791097829508227, 0.0279356601626374, 0.026661764743716218, 0.0268526132716689, 0.0259406705235326, 0.02549608918123467, 0.025686652161354243, 0.025290494741395463, 0.024247990098110465, 0.02342583135116932, 0.023766772691593614, 0.022722884666088017, 0.022170424461364746, 0.021233489347058675, 0.021158867104108945, 0.019961207412010015, 0.019394076147744823, 0.018816016441167788, 0.01826383069504139, 0.0184833615325218, 0.017661740613538167, 0.017551047857417616, 0.018508079440094704, 0.01821238218351852, 0.017638630645219668, 0.017520050669825354, 0.017432276592698207, 0.018199765404989552, 0.019990580026493517, 0.019809487254120583, 0.019843650418658588, 0.016481252603752668, 0.01666606858719227, 0.014629480450652366, 0.015072193256644316, 0.0178592371386151, 0.0157509820405827, 0.017464598944020825, 0.014901075252266817, 0.01580891221068626, 0.014183022255121275, 0.014920653298843739, 0.015740968460260435, 0.017044408376826796, 0.014967264131058094, 0.016175810680832972, 0.01820317534513252, 0.017868818238724108, 0.01691629443057748, 0.017983686092287995, 0.016256964483926464, 0.018454845561537633, 0.018086580343024676, 0.017778673837351246, 0.017464474190113155, 0.018230316250823265, 0.017049520514732183, 0.01746531419975813]\n"
     ]
    }
   ],
   "source": [
    "print(loss_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.1854]) tensor([2.1956])\n",
      "tensor([2.5116]) tensor([2.4790])\n",
      "tensor([2.3120]) tensor([2.3395])\n",
      "tensor([2.3154]) tensor([2.3422])\n",
      "tensor([2.4350]) tensor([2.4667])\n",
      "tensor([2.3133]) tensor([2.3406])\n",
      "tensor([2.0516]) tensor([2.0610])\n",
      "tensor([2.6355]) tensor([2.6542])\n",
      "tensor([2.3123]) tensor([2.3299])\n",
      "tensor([2.3084]) tensor([2.3289])\n",
      "tensor([2.0539]) tensor([2.0650])\n",
      "tensor([2.0288]) tensor([2.0358])\n",
      "tensor([2.0146]) tensor([2.0102])\n",
      "tensor([2.0023]) tensor([2.0231])\n",
      "tensor([2.6274]) tensor([2.6402])\n",
      "tensor([2.3035]) tensor([2.3338])\n",
      "tensor([1.8940]) tensor([1.8888])\n",
      "tensor([2.2991]) tensor([2.3194])\n",
      "tensor([1.8690]) tensor([1.8731])\n",
      "tensor([2.2934]) tensor([2.3109])\n",
      "tensor([1.8510]) tensor([1.8471])\n",
      "tensor([2.2940]) tensor([2.3207])\n",
      "tensor([2.2310]) tensor([2.3072])\n",
      "tensor([1.8050]) tensor([1.8193])\n",
      "tensor([1.7948]) tensor([1.8112])\n",
      "tensor([2.2739]) tensor([2.2663])\n",
      "tensor([1.7846]) tensor([1.8145])\n",
      "tensor([1.7818]) tensor([1.7895])\n",
      "tensor([2.2775]) tensor([2.2878])\n",
      "tensor([1.7573]) tensor([1.7871])\n",
      "tensor([1.7343]) tensor([1.7696])\n",
      "tensor([2.2558]) tensor([2.2663])\n",
      "tensor([2.2563]) tensor([2.2658])\n",
      "tensor([2.2546]) tensor([2.2548])\n",
      "tensor([2.2569]) tensor([2.2801])\n",
      "tensor([2.2288]) tensor([2.2211])\n",
      "tensor([2.2257]) tensor([2.2334])\n",
      "tensor([2.2249]) tensor([2.2417])\n",
      "tensor([2.2178]) tensor([2.2317])\n",
      "tensor([2.2113]) tensor([2.2338])\n",
      "tensor([2.5405]) tensor([2.5241])\n",
      "tensor([2.5302]) tensor([2.5239])\n",
      "tensor([2.5254]) tensor([2.5224])\n"
     ]
    }
   ],
   "source": [
    "#evaluate model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(test_tensor)):\n",
    "            \n",
    "        test_out = model((test_tensor[i].unsqueeze(0)).float())\n",
    "        test_loss = criterion(test_out, test_label[i])\n",
    "        print(test_label[i], test_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.2862]) tensor(2.2638)\n",
      "tensor([2.2906]) tensor(2.3471)\n",
      "tensor([2.3079]) tensor(2.3482)\n",
      "tensor([2.3081]) tensor(2.3407)\n",
      "tensor([2.3189]) tensor(2.3535)\n",
      "tensor([2.3150]) tensor(2.3480)\n",
      "tensor([2.3246]) tensor(2.3556)\n",
      "tensor([2.3270]) tensor(2.3497)\n",
      "tensor([2.3241]) tensor(2.3501)\n",
      "tensor([2.3220]) tensor(2.3530)\n",
      "tensor([2.3158]) tensor(2.3446)\n",
      "tensor([2.3217]) tensor(2.3481)\n",
      "tensor([2.3225]) tensor(2.3551)\n",
      "tensor([2.3225]) tensor(2.3437)\n",
      "tensor([2.3321]) tensor(2.3540)\n",
      "tensor([2.6468]) tensor(2.7344)\n",
      "tensor([2.3245]) tensor(2.3339)\n",
      "tensor([2.3194]) tensor(2.3500)\n",
      "tensor([2.3210]) tensor(2.3503)\n",
      "tensor([2.3184]) tensor(2.3358)\n",
      "tensor([2.3203]) tensor(2.3377)\n",
      "tensor([2.3169]) tensor(2.3402)\n",
      "tensor([2.3198]) tensor(2.3498)\n",
      "tensor([2.6437]) tensor(2.6833)\n",
      "tensor([2.3209]) tensor(2.3494)\n",
      "tensor([2.3117]) tensor(2.3407)\n",
      "tensor([2.3193]) tensor(2.3542)\n",
      "tensor([2.3116]) tensor(2.3472)\n",
      "tensor([2.6385]) tensor(2.6724)\n",
      "tensor([2.3202]) tensor(2.3359)\n",
      "tensor([2.3146]) tensor(2.3262)\n",
      "tensor([2.6351]) tensor(2.6658)\n",
      "tensor([2.3189]) tensor(2.3477)\n",
      "tensor([2.3147]) tensor(2.3447)\n",
      "tensor([2.3087]) tensor(2.3346)\n",
      "tensor([2.3142]) tensor(2.3385)\n",
      "tensor([2.2975]) tensor(2.3157)\n",
      "tensor([2.3024]) tensor(2.3135)\n",
      "tensor([2.3053]) tensor(2.3391)\n",
      "tensor([2.6251]) tensor(2.6553)\n",
      "tensor([2.3110]) tensor(2.3251)\n",
      "tensor([2.3093]) tensor(2.3250)\n",
      "tensor([2.3089]) tensor(2.3262)\n",
      "tensor([2.6254]) tensor(2.6543)\n",
      "tensor([2.3064]) tensor(2.3254)\n",
      "tensor([2.3019]) tensor(2.3263)\n",
      "tensor([2.3047]) tensor(2.3084)\n",
      "tensor([2.6221]) tensor(2.6268)\n",
      "tensor([2.3007]) tensor(2.3098)\n",
      "tensor([2.2985]) tensor(2.3327)\n",
      "tensor([2.6163]) tensor(2.6252)\n",
      "tensor([2.2983]) tensor(2.3328)\n",
      "tensor([2.2970]) tensor(2.3072)\n",
      "tensor([2.2933]) tensor(2.3277)\n",
      "tensor([2.2906]) tensor(2.3103)\n",
      "tensor([2.2868]) tensor(2.2949)\n",
      "tensor([2.2666]) tensor(2.2786)\n",
      "tensor([2.2818]) tensor(2.2943)\n",
      "tensor([2.6010]) tensor(2.5885)\n",
      "tensor([2.2773]) tensor(2.2827)\n",
      "tensor([2.2773]) tensor(2.2836)\n",
      "tensor([2.5960]) tensor(2.6226)\n",
      "tensor([2.2813]) tensor(2.2931)\n",
      "tensor([2.2712]) tensor(2.2751)\n",
      "tensor([2.2785]) tensor(2.3071)\n",
      "tensor([2.5971]) tensor(2.5889)\n",
      "tensor([2.2793]) tensor(2.2912)\n",
      "tensor([2.2776]) tensor(2.2745)\n",
      "tensor([2.5937]) tensor(2.5261)\n",
      "tensor([2.2743]) tensor(2.2913)\n",
      "tensor([2.2709]) tensor(2.2629)\n",
      "tensor([2.2716]) tensor(2.2757)\n",
      "tensor([2.2659]) tensor(2.2925)\n",
      "tensor([2.5845]) tensor(2.5675)\n",
      "tensor([2.2630]) tensor(2.2647)\n",
      "tensor([2.2538]) tensor(2.2855)\n",
      "tensor([2.5830]) tensor(2.5677)\n",
      "tensor([2.2638]) tensor(2.2748)\n",
      "tensor([2.2477]) tensor(2.2570)\n",
      "tensor([2.2525]) tensor(2.2558)\n",
      "tensor([2.2434]) tensor(2.2667)\n",
      "tensor([2.2478]) tensor(2.2563)\n",
      "tensor([2.2461]) tensor(2.2648)\n",
      "tensor([2.5698]) tensor(2.5466)\n",
      "tensor([2.2445]) tensor(2.2579)\n",
      "tensor([2.2401]) tensor(2.2420)\n",
      "tensor([2.2397]) tensor(2.2431)\n",
      "tensor([2.2320]) tensor(2.2593)\n",
      "tensor([2.2367]) tensor(2.2320)\n",
      "tensor([2.5613]) tensor(2.5668)\n",
      "tensor([2.2367]) tensor(2.2542)\n",
      "tensor([2.5593]) tensor(2.5684)\n",
      "tensor([2.2353]) tensor(2.2394)\n",
      "tensor([2.2346]) tensor(2.2297)\n",
      "tensor([2.2294]) tensor(2.2177)\n",
      "tensor([2.2209]) tensor(2.2300)\n",
      "tensor([2.2161]) tensor(2.2213)\n",
      "tensor([2.2094]) tensor(2.2319)\n",
      "tensor([2.2070]) tensor(2.2065)\n",
      "tensor([2.2042]) tensor(2.2069)\n",
      "tensor([2.1998]) tensor(2.2216)\n",
      "tensor([2.1875]) tensor(2.1965)\n",
      "tensor([2.5205]) tensor(2.4786)\n",
      "tensor([2.1773]) tensor(2.1970)\n",
      "tensor([2.1810]) tensor(2.1937)\n",
      "tensor([2.5158]) tensor(2.4806)\n",
      "tensor([2.1781]) tensor(2.1702)\n",
      "tensor([2.1671]) tensor(2.1952)\n",
      "tensor([2.1706]) tensor(2.1698)\n",
      "tensor([2.1584]) tensor(2.1614)\n",
      "tensor([2.5017]) tensor(2.4786)\n",
      "tensor([2.1607]) tensor(2.1577)\n",
      "tensor([2.1614]) tensor(2.1702)\n",
      "tensor([2.1548]) tensor(2.1582)\n",
      "tensor([2.1542]) tensor(2.1684)\n",
      "tensor([2.1536]) tensor(2.1552)\n",
      "tensor([2.1464]) tensor(2.1726)\n",
      "tensor([2.1420]) tensor(2.1293)\n",
      "tensor([2.1298]) tensor(2.1221)\n",
      "tensor([2.1190]) tensor(2.1323)\n",
      "tensor([2.1128]) tensor(2.1323)\n",
      "tensor([2.4597]) tensor(2.5007)\n",
      "tensor([2.1090]) tensor(2.1184)\n",
      "tensor([2.0837]) tensor(2.0851)\n",
      "tensor([2.0758]) tensor(2.0555)\n",
      "tensor([2.4336]) tensor(2.5071)\n",
      "tensor([2.0585]) tensor(2.0438)\n",
      "tensor([2.0549]) tensor(2.0385)\n",
      "tensor([2.0579]) tensor(2.0464)\n",
      "tensor([2.0658]) tensor(2.0686)\n",
      "tensor([2.0685]) tensor(2.0722)\n",
      "tensor([2.0527]) tensor(2.0529)\n",
      "tensor([2.0620]) tensor(2.0724)\n",
      "tensor([2.0529]) tensor(2.0489)\n",
      "tensor([2.0502]) tensor(2.0547)\n",
      "tensor([2.0458]) tensor(2.0577)\n",
      "tensor([2.4020]) tensor(2.3917)\n",
      "tensor([2.0411]) tensor(2.0677)\n",
      "tensor([2.0450]) tensor(2.0489)\n",
      "tensor([2.0420]) tensor(2.0498)\n",
      "tensor([2.3848]) tensor(2.3754)\n",
      "tensor([2.3543]) tensor(2.3678)\n",
      "tensor([1.9856]) tensor(2.0029)\n",
      "tensor([1.9929]) tensor(2.0129)\n",
      "tensor([1.9829]) tensor(1.9809)\n",
      "tensor([1.9798]) tensor(1.9993)\n",
      "tensor([1.9548]) tensor(1.9707)\n",
      "tensor([1.9608]) tensor(1.9702)\n",
      "tensor([2.3121]) tensor(2.3838)\n",
      "tensor([1.9245]) tensor(1.9421)\n",
      "tensor([1.9045]) tensor(1.9049)\n",
      "tensor([1.8965]) tensor(1.8971)\n",
      "tensor([1.8749]) tensor(1.8602)\n",
      "tensor([2.2691]) tensor(2.3116)\n",
      "tensor([2.2535]) tensor(2.2997)\n",
      "tensor([2.2403]) tensor(2.2855)\n",
      "tensor([1.8459]) tensor(1.8556)\n",
      "tensor([1.8439]) tensor(1.8404)\n",
      "tensor([1.8403]) tensor(1.8470)\n",
      "tensor([1.8290]) tensor(1.8454)\n",
      "tensor([2.2029]) tensor(2.2940)\n",
      "tensor([1.8168]) tensor(1.8175)\n",
      "tensor([1.8040]) tensor(1.8209)\n",
      "tensor([1.7920]) tensor(1.8011)\n",
      "tensor([2.1777]) tensor(2.3161)\n",
      "tensor([2.1662]) tensor(2.3217)\n",
      "tensor([1.7700]) tensor(1.8003)\n",
      "tensor([1.7635]) tensor(1.7997)\n",
      "tensor([1.7415]) tensor(1.7698)\n",
      "tensor([1.7572]) tensor(1.7776)\n",
      "tensor([2.1184]) tensor(2.3084)\n",
      "tensor(0.1900)\n"
     ]
    }
   ],
   "source": [
    "#evaluate model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(train_tensor)):\n",
    "            \n",
    "        train_out = model((train_tensor[i].unsqueeze(0)).float())\n",
    "        train_loss = criterion(train_out, train_label[i])\n",
    "        print(train_label[i], train_out[-1])\n",
    "print (train_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/apk/84bd6553-82dd-4faa-9701-e84d64d75ed7/anaconda3/lib/python3.7/site-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type LSTMNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, \"/home/apk/LSTMmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/home/apk/LSTMmodelstate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss\n",
    "            },\"/home/apk/LSTMmodelstate_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
